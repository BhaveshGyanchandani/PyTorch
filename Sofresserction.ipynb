{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SoftResurrectReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SoftResurrectReLU Activation:\n",
    "    - Retains ReLU-like behavior for positive values\n",
    "    - Allows small gradient flow for negative inputs via a smooth curve\n",
    "    - Designed to prevent neuron death and enhance long training stability\n",
    "    - works like a beast for very low lr like 1e-3 to 1e-6 and so on......\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, beta=0.5):\n",
    "        super(SoftResurrectReLU, self).__init__()\n",
    "        self.alpha = alpha  # controls sharpness of resurrection\n",
    "        self.beta = beta    # controls contribution from the negative side\n",
    "\n",
    "    def forward(self, x):\n",
    "        positive = F.relu(x)  # standard ReLU for x >= 0\n",
    "        negative = self.beta * torch.tanh(self.alpha * x)  # smooth curve for x < 0\n",
    "        return torch.where(x >= 0, positive, negative)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
