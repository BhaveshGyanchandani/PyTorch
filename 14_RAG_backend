# backend_rag.py
from fastapi import FastAPI, Request
from pydantic import BaseModel
from typing import List
from dotenv import load_dotenv
import os
import requests
import json
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

import os
from datetime import datetime
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

# ----------------------------
# 1Ô∏è‚É£ Load environment variables
# ----------------------------
dotenv_path=r"D:\web dev backup\React_native\ShenAI\.env"
load_dotenv(dotenv_path)
PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')
os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
# ----------------------------
# 2Ô∏è‚É£ Initialize embeddings
# ----------------------------
embeddings = HuggingFaceBgeEmbeddings(
    model_name=r"D:\web dev backup\Pytorch\bge-large-en-v1.5"
)


uri = "mongodb+srv://Bhavesh:AJstylesP1@cluster0.u4rgl.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"

# Create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))

# Send a ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)
    
# ----------------------------
# 3Ô∏è‚É£ Initialize Pinecone and vectorstore
# ----------------------------
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
index_name = "sample-data-1024"

docsearch = PineconeVectorStore.from_existing_index(
    index_name=index_name,
    embedding=embeddings
)
retriever = docsearch.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

system_prompt = (
    "You are an Medical assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)

query_filter = (
        "You are a strict grammar correction engine. "
        "correct the misspelled words in query and return the corrected query ."
        "Dont provide any extra text ."
        "If the query doesnt contain any errors then provide the same query as output ."
    )



# ----------------------------
# 4Ô∏è‚É£ Ollama query function
# ----------------------------
def query_ollama(messages: List[dict], model="llava:7b-v1.6-mistral-q4_0") -> str:
    """
    Send messages to local Ollama LLava model and return combined response.
    """
    url = "http://localhost:11434/api/chat"
    payload = {
        "model": model,
        "messages": messages
    }

    response = requests.post(url, json=payload, stream=True)
    if response.status_code == 200:
        full_text = ""
        for line in response.iter_lines(decode_unicode=True):
            if line:
                try:
                    json_data = json.loads(line)
                    if "message" in json_data and "content" in json_data["message"]:
                        full_text += json_data["message"]["content"]
                except json.JSONDecodeError:
                    continue
        return full_text
    else:
        return f"Request failed: {response.status_code} - {response.text}"

# ----------------------------
# 5Ô∏è‚É£ FastAPI app
# ----------------------------
app = FastAPI(title="RAG + Ollama Backend")

# ----------------------------
# 6Ô∏è‚É£ Request model
# ----------------------------
class QueryRequest(BaseModel):
    query: str

# ----------------------------
# 7Ô∏è‚É£ POST endpoint
# ----------------------------


db=client["MedicalDB"]

collection = db["Shen"]



@app.post("/data")
async def process_query(request: QueryRequest):
    Initial_query = request.query.strip()
    
    corrected_messages = [
        {"role": "system", "content": query_filter},
        {"role": "user", "content": f"User question: {Initial_query}"}
    ]
    
    query_text = query_ollama(corrected_messages)

    # corrected_messages = client.chat.completions.create(
    #     model="gpt-4o",
    #     messages=[
    #         {"role": "system", "content": query_filter},
    #         {"role": "user", "content": Initial_query}
    #     ]
    # )
    
    # query_text = corrected_messages.choices[0].message.content.strip()
    
    print(f"corrected query is : {query_text}")

    # 1Ô∏è‚É£ Retrieve relevant docs from Pinecone
    docs_and_scores = retriever.vectorstore.similarity_search_with_score(query_text, k=3)

    # 2Ô∏è‚É£ Filter by score > 0.75
    filtered_docs = [doc for doc, score in docs_and_scores if score > 0.6]

    if not filtered_docs:
    #     answer ="No relevant match found in Pinecone."
    #     AddDocToDB = [
    #     {"query": Initial_query, "Answer": answer, "DateTime":datetime.now() }
    # ]
    #     result = collection.insert_many(AddDocToDB)
        
    #     print("Inserted document IDs:", result.inserted_ids)
        return {"summary": "No relevant match found in Pinecone."}

    # 3Ô∏è‚É£ Combine context text
    context_text = "\n".join([doc.page_content for doc in filtered_docs])

    # 4Ô∏è‚É£ Prepare messages for Ollama
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{context_text}\n\nUser question: {query_text}"}
    ]

    # 5Ô∏è‚É£ Query Ollama
    answer = query_ollama(messages)
    
    # AddDocToDB = [
    #     {"query": Initial_query, "Answer": answer, "DateTime":datetime.now() }
    # ]
    # result = collection.insert_many(AddDocToDB)
    
    # print("Inserted document IDs:", result.inserted_ids)
    # 6Ô∏è‚É£ Return the result
    return {"summary": answer}

import uvicorn
if __name__ == "__main__":
    print("üöÄ Backend is running on http://localhost:8000")
    uvicorn.run(
        app, 
        host="localhost",  # listens only on your machine
        port=8000,         # the port you fetch from in RN
        timeout_keep_alive=30  # optional, helps with long-running requests
    )

