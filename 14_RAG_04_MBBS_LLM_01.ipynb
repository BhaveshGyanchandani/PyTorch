{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MM foe video processign and Q,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ceda2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-lancedb\n",
    "# %pip install llama-index-multi-modal-llms-openai\n",
    "# %pip install llama-index-embeddings-clip\n",
    "# %pip install git+https://github.com/openai/CLIP.git\n",
    "# %pip install llama-index-readers-file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4bed452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama index\n",
    "# %pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2de5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lancedb\n",
    "# %pip install moviepy\n",
    "# %pip install pytube\n",
    "# %pip install pydub\n",
    "# %pip install SpeechRecognition\n",
    "# %pip install ffmpeg-python\n",
    "# %pip install soundfile\n",
    "# %pip install torch torchvision\n",
    "# %pip install matplotlib scikit-image\n",
    "# %pip install ftfy regex tqdm\n",
    "\n",
    "\n",
    "# %pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bc0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-openai\n",
      "  Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl.metadata (400 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-embeddings-openai) (0.13.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-embeddings-openai) (1.102.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.11.14)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\bhavesh\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\bhavesh\\appdata\\roaming\\python\\python312\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (4.3.6)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.18.3)\n",
      "Requirement already satisfied: griffe in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.13.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.10)\n",
      "Requirement already satisfied: click in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2024.11.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhavesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-openai) (3.0.2)\n",
      "Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: llama-index-embeddings-openai\n",
      "Successfully installed llama-index-embeddings-openai-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37ec5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85130de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import speech_recognition as sr\n",
    "from pytube import YouTube\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18837386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b67239d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://youtu.be/3dhcmeOTZ_Q?si=5Xbb2sqsFQZqGXSl\"\n",
    "output_video_path = \"./RAG4_MBBS_MM/video_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12c4d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the video i am going to collect images , audio text,images/\n",
    "output_folder = \"./RAG4_MBBS_MM/mixed_data/\"\n",
    "output_audio_path=\"./RAG4_MBBS_MM/mixed_data/output_audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43faa82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./RAG4_MBBS_MM/video_data/input_vid.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filepath = os.path.join(output_video_path, \"input_vid.mp4\")\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87117fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytube import YouTube\n",
    "\n",
    "# def download_video(url,output_video_path):\n",
    "#     yt = YouTube(url)\n",
    "#     metadata = {\"Author\":yt.author,\n",
    "#                 \"Title\":yt.title,\n",
    "#                 \"views\": yt.views}\n",
    "    \n",
    "#     yt.streams.get_highest_resolution().download(\\\n",
    "#         output_path=output_video_path,\n",
    "#         filename=\"input_vid.mp4\")\n",
    "    \n",
    "#     return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import yt_dlp\n",
    "import os, json\n",
    "\n",
    "def download_video(url, output_path):\n",
    "    # ensure folder exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # yt-dlp options\n",
    "    ydl_opts = {\n",
    "        'outtmpl': os.path.join(output_path, 'input_vid.mp4'),\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "    \n",
    "    metadata = {\n",
    "        \"Author\": result.get(\"uploader\", \"Unknown\"),\n",
    "        \"Title\": result.get(\"title\", \"Unknown Title\"),\n",
    "        \"Views\": result.get(\"view_count\", \"Unknown Views\"),\n",
    "    }\n",
    "\n",
    "    # save metadata as JSON\n",
    "    meta_path = os.path.join(output_path, \"input_vid.json\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f629ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect of images from videos ( since videos constit of frames or images)\n",
    "\n",
    "def video_to_images(video_path,output_folder):\n",
    "    clip=VideoFileClip(video_path)\n",
    "    clip.write_images_sequence(\n",
    "        os.path.join(output_folder,\n",
    "                     \"frame%04d.png\"),\n",
    "        fps=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15c21738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_audio(video_path,output_audio_path):\n",
    "    clip=VideoFileClip(video_path)\n",
    "    audio=clip.audio\n",
    "    audio.write_audiofile(output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ecf9b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    audio = sr.AudioFile(audio_path)\n",
    "    \n",
    "    with audio as source:\n",
    "        audio_data =recognizer.record(source)\n",
    "        \n",
    "        try:\n",
    "            # to recognixe speech\n",
    "            text = recognizer.recognize_whisper(audio_data)\n",
    "        \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"speech recogniztion couldnt understand the audio.\")\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d0b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/3dhcmeOTZ_Q?si=5Xbb2sqsFQZqGXSl\n",
      "[youtube] 3dhcmeOTZ_Q: Downloading webpage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] 3dhcmeOTZ_Q: Downloading tv simply player API JSON\n",
      "[youtube] 3dhcmeOTZ_Q: Downloading tv client config\n",
      "[youtube] 3dhcmeOTZ_Q: Downloading tv player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] 3dhcmeOTZ_Q: Downloading 1 format(s): 18\n",
      "[download] RAG4_MBBS_MM\\video_data\\input_vid.mp4 has already been downloaded\n",
      "[download] 100% of    5.80MiB\n"
     ]
    }
   ],
   "source": [
    "metadata_vid = download_video(video_url, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737973a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Author': '3-Minute Data Science',\n",
       " 'Title': 'Linear Regression in 3 Minutes',\n",
       " 'Views': 57896}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67dfa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Writing frames ./RAG4_MBBS_MM/mixed_data/frame%04d.png.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done writing frames ./RAG4_MBBS_MM/mixed_data/frame%04d.png.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# video_to_images(filepath,output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc78a83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_video_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# path to your saved file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m filepath = os.path.join(\u001b[43moutput_video_path\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33minput_vid.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# metadata you already have\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVideo Metadata:\u001b[39m\u001b[33m\"\u001b[39m, metadata_vid)\n",
      "\u001b[31mNameError\u001b[39m: name 'output_video_path' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# path to your saved file\n",
    "filepath = os.path.join(output_video_path, \"input_vid.mp4\")\n",
    "\n",
    "# metadata you already have\n",
    "print(\"Video Metadata:\", metadata_vid)\n",
    "\n",
    "# now load the video without downloading again\n",
    "if os.path.exists(filepath):\n",
    "    print(\"Video file exists:\", filepath)\n",
    "else:\n",
    "    print(\"File not found. Maybe wrong filename?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2a723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./RAG4_MBBS_MM/mixed_data/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# video_to_audio(filepath,output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37792575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [01:10<00:00, 2.06MiB/s]\n"
     ]
    }
   ],
   "source": [
    "text_data = audio_to_text(output_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" When you regression, as a statistical technique for modeling the relationship between an output variable and one or more input variables, in layman's terms, think of it as fitting a line through some data points as shown here so you can make predictions on unknown data, assuming there is a linear relationship between the variables. You might be familiar with the linear function y equals mx plus b, where y is the output variable, also called the dependent variable. You may also see expressed as f of x, the function of the input variable. x on the other hand would serve as the input variable, also called the independent variable. It's likely you'll see the coefficients m and b expressed as beta 1 and beta 0 respectively. So what do the m and b coefficients do? The m or beta 1 coefficient controls the slope of the line, the b or the beta 0 controls the intercept of the line, in machine learning we also know it as the bias. These two coefficients are what we are solving for in linear regression. We can also extend to multiple input variables, so x1, x2, x3, with beta 1, beta 2, and beta 3, and so on, acting as slopes for each of those variables. In these higher dimensions, you would visualize the linear regression as a hyperplane. So how do we fit the line to these points? Well you'll notice that there's these differences between the points and the line, these little red segments, these are called residuals. They are the differences between the data points and the predictions the line would produce. Take each of these residuals and square them. These are the squared errors, and notice that the large of the residuals are, the more amplified area of the squares are. If we total the areas of all of these squares for a given line, we will get the sum of the squared error, and this is known as our loss function. We need to find the beta 0 and beta 1 coefficients that will minimize that sum of squared error. The coefficients can be solved with a variety of techniques ranging from matrix decomposition to gradient descent, which is depicted right here. Thankfully a lot of libraries are available to do this for us, and we will deep dive into these topics in other videos. To validate a linear regression, there are a number of techniques. Machine learning practitioners will often take a third of the data and put it into the test data set. The remaining two thirds will become the training data set. The training data set will then be used to fit the regression line. The test data set will then be used to validate the regression line. This is done to make sure that the regression performs well on data it has not seen before. The tricks used to evaluate the linear regression vary from the R square, standard error of the estimate, prediction intervals, as well as statistical significance. These are topics we will cover in future videos. If you enjoyed this video, please like and subscribe. Look at my two O'Reilly books, Essential Math for Data Science, and getting started with SQL. Chapter five of Essential Math for Data Science actually covers linear regression and much more depth. If you want live instruction, I also do teach on the O'Reilly platform, promotional link below. I teach classes including machine learning from scratch, probability, and SQL. Comment on what topics you would like to see next, and I will see you again on 3 Minute Data Science.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df695bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text data saved to file\n"
     ]
    }
   ],
   "source": [
    "with open(output_folder+\"output_text.txt\" ,\"w\") as file:\n",
    "    file.write(text_data)\n",
    "\n",
    "print(\"text data saved to file\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a906705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4273a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18b4586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Table text_collection doesn't exist yet. Please add some data to create it.\n",
      "Table image_collection doesn't exist yet. Please add some data to create it.\n"
     ]
    }
   ],
   "source": [
    "text_store = LanceDBVectorStore(uri = \"lancedb\", table_name=\"text_collection\")\n",
    "image_store = LanceDBVectorStore(uri=\"lancedb\",table_name=\"image_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ff7d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=text_store,image_store=image_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fafe7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(output_folder).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "010c8b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 21:31:14,715 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:31:14,716 - INFO - Retrying request to /embeddings in 0.423465 seconds\n",
      "2025-08-29 21:31:17,173 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:31:17,174 - INFO - Retrying request to /embeddings in 0.846097 seconds\n",
      "2025-08-29 21:31:20,632 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:31:20,634 - INFO - Retrying request to /embeddings in 1.927642 seconds\n",
      "2025-08-29 21:31:24,544 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:31:24,545 - INFO - Retrying request to /embeddings in 3.823651 seconds\n",
      "2025-08-29 21:31:30,720 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:31:30,721 - INFO - Retrying request to /embeddings in 7.163946 seconds\n",
      "2025-08-29 21:31:40,876 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:31:40,878 - INFO - Retrying request to /embeddings in 6.129915 seconds\n",
      "2025-08-29 21:31:50,701 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:31:50,702 - INFO - Retrying request to /embeddings in 7.026736 seconds\n",
      "2025-08-29 21:32:00,891 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:32:00,891 - INFO - Retrying request to /embeddings in 7.035458 seconds\n",
      "2025-08-29 21:32:10,372 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:32:10,373 - INFO - Retrying request to /embeddings in 6.089229 seconds\n",
      "2025-08-29 21:32:19,409 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-29 21:32:19,410 - INFO - Retrying request to /embeddings in 7.568128 seconds\n",
      "2025-08-29 21:32:29,843 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m index = \u001b[43mMultiModalVectorStoreIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:122\u001b[39m, in \u001b[36mBaseIndex.from_documents\u001b[39m\u001b[34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     docstore.set_document_hash(doc.id_, doc.hash)\n\u001b[32m    115\u001b[39m nodes = run_transformations(\n\u001b[32m    116\u001b[39m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    117\u001b[39m     transformations,\n\u001b[32m    118\u001b[39m     show_progress=show_progress,\n\u001b[32m    119\u001b[39m     **kwargs,\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\multi_modal\\base.py:105\u001b[39m, in \u001b[36mMultiModalVectorStoreIndex.__init__\u001b[39m\u001b[34m(self, nodes, index_struct, embed_model, storage_context, use_async, store_nodes_override, show_progress, image_vector_store, image_embed_model, is_image_to_text, is_image_vector_store_empty, is_text_vector_store_empty, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     storage_context.add_vector_store(SimpleVectorStore(), \u001b[38;5;28mself\u001b[39m.image_namespace)\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m._image_vector_store = storage_context.vector_stores[\u001b[38;5;28mself\u001b[39m.image_namespace]\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore_nodes_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore_nodes_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:75\u001b[39m, in \u001b[36mVectorStoreIndex.__init__\u001b[39m\u001b[34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m._embed_model = resolve_embed_model(\n\u001b[32m     71\u001b[39m     embed_model \u001b[38;5;129;01mor\u001b[39;00m Settings.embed_model, callback_manager=callback_manager\n\u001b[32m     72\u001b[39m )\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m._insert_batch_size = insert_batch_size\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:79\u001b[39m, in \u001b[36mBaseIndex.__init__\u001b[39m\u001b[34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     nodes = nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     index_struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m._index_struct = index_struct\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_context.index_store.add_index_struct(\u001b[38;5;28mself\u001b[39m._index_struct)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:309\u001b[39m, in \u001b[36mVectorStoreIndex.build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **insert_kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content_nodes) != \u001b[38;5;28mlen\u001b[39m(nodes):\n\u001b[32m    307\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSome nodes are missing content, skipping them...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:278\u001b[39m, in \u001b[36mVectorStoreIndex._build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **insert_kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     run_async_tasks(tasks)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\multi_modal\\base.py:385\u001b[39m, in \u001b[36mMultiModalVectorStoreIndex._add_nodes_to_index\u001b[39m\u001b[34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m         text_nodes.append(node)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text_nodes) > \u001b[32m0\u001b[39m:\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# embed all nodes as text - include image nodes that have text attached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     text_nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_image\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m     new_text_ids = \u001b[38;5;28mself\u001b[39m.storage_context.vector_stores[DEFAULT_VECTOR_STORE].add(\n\u001b[32m    389\u001b[39m         text_nodes, **insert_kwargs\n\u001b[32m    390\u001b[39m     )\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\multi_modal\\base.py:225\u001b[39m, in \u001b[36mMultiModalVectorStoreIndex._get_node_with_embedding\u001b[39m\u001b[34m(self, nodes, show_progress, is_image)\u001b[39m\n\u001b[32m    222\u001b[39m         \u001b[38;5;28mself\u001b[39m._image_embed_model = \u001b[38;5;28mself\u001b[39m._embed_model  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     id_to_embed_map = \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m results = []\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\indices\\utils.py:176\u001b[39m, in \u001b[36membed_nodes\u001b[39m\u001b[34m(nodes, embed_model, show_progress)\u001b[39m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m         id_to_embed_map[node.node_id] = node.embedding\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m new_embeddings = \u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[32m    181\u001b[39m     id_to_embed_map[new_id] = text_embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:317\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    320\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\core\\base\\embeddings\\base.py:473\u001b[39m, in \u001b[36mBaseEmbedding.get_text_embedding_batch\u001b[39m\u001b[34m(self, texts, show_progress, **kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    469\u001b[39m     CBEventType.EMBEDDING,\n\u001b[32m    470\u001b[39m     payload={EventPayload.SERIALIZED: \u001b[38;5;28mself\u001b[39m.to_dict()},\n\u001b[32m    471\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings_cache:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m         embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    475\u001b[39m         embeddings = \u001b[38;5;28mself\u001b[39m._get_text_embeddings_cached(cur_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:472\u001b[39m, in \u001b[36mOpenAIEmbedding._get_text_embeddings\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retryable_get_embeddings\u001b[39m():\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[32m    466\u001b[39m         client,\n\u001b[32m    467\u001b[39m         texts,\n\u001b[32m    468\u001b[39m         engine=\u001b[38;5;28mself\u001b[39m._text_engine,\n\u001b[32m    469\u001b[39m         **\u001b[38;5;28mself\u001b[39m.additional_kwargs,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retryable_get_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:465\u001b[39m, in \u001b[36mOpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retryable_get_embeddings\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_text_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:172\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(client, list_of_text, engine, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) <= \u001b[32m2048\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe batch size should not be larger than 2048.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m list_of_text = [text.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m data = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.data\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [d.embedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "index = MultiModalVectorStoreIndex.from_documents(documents,storage_context = storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(output_folder).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123aa6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = MultiModalVectorStoreIndex.from_documents(documents,storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f370a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_engine=  index.as_retriever(similarity_top_k=1,image_similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tmpl_str=(\n",
    "    \"Based on provided information , including relevant images and retrieved context from the video , accuractely and precisely answer the query withot any additonal prior knowledge.\\n\"\n",
    "    \n",
    "    \"-----------------\\n\"\n",
    "    \"Context:{context_str}\\n\"\n",
    "    \"Metadata for video : {metadata_str}\\n\"\n",
    "    \n",
    "    \"------------------\\n\"\n",
    "    \"Query : {query_str}\\n\"\n",
    "    \"Answer : \"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8419782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c868b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(retriever_engine,query_str):\n",
    "    \n",
    "    retrieval_results = retriever_engine.retrieve(query_str)\n",
    "    retrieved_image = []\n",
    "    retrieved_text = []\n",
    "    \n",
    "    for res_node in retrieval_results:\n",
    "        \n",
    "        if isinstance(res_node.node,ImageNode):\n",
    "            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        \n",
    "        else:\n",
    "            display_source_node(res_node,source_length=200)\n",
    "            retrieved_text.append(res_node.text)\n",
    "            \n",
    "    \n",
    "    return retrieved_image,retrieved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"whats linear regression ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, text = retrieve(retriever_engine,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df36583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(images_path):\n",
    "    images_shown = 0\n",
    "    \n",
    "    plt.figure(figsize=(16,9))\n",
    "    \n",
    "    for img_path in images_path:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "            \n",
    "            plt.subplot(2,3,images_shown+1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            \n",
    "            images_shown +=1\n",
    "            if images_shown >=5:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c26ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352065a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "metadata_str = json.dumps(metadata_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_str = \"\".join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_documents = SimpleDirectoryReader(input_files=img).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"explain me linear regression and book for it used .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ebd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openAIMultiModal = OpenAIMultiModal(model=\"gpt-4-vision-preview\",api_key=OPENAI_API_KEY , max_token=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd10f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = openAIMultiModal.complete(\n",
    "    prompt= qa_tmpl_str.format(\n",
    "        query_str = query_str,\n",
    "        metadata_str=metadata_str,\n",
    "        context_str=context_str\n",
    "    ),\n",
    "    image_documents=image_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62119c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(result.text) # pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30201ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"You are an assistant tasked with summarizing text for retrieval. \n",
    "These summaries will be embedded and used to retrieve the raw text elements. \n",
    "Give a concise summary of the table or text that is well optimized for retrieval.text: {element} \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a56596",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty summaries\n",
    "table_summaries = []\n",
    "table_summaries = summarize_chain.batch(Table, {\"max_concurrency\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty summaries\n",
    "text_summaries = []\n",
    "text_summaries = summarize_chain.batch(Text, {\"max_concurrency\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e0da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906281f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return msg.content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
