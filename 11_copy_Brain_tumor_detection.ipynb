{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, pad_list_data_collate\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    Spacingd,\n",
    "    ToTensord,\n",
    "    SpatialCropd,\n",
    "    RandAdjustContrastd,\n",
    "    RandGaussianNoised,\n",
    "    RandCoarseShuffled,\n",
    "    CropForegroundd,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu' )\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelabelTransform(MapTransform):\n",
    "    def __init__(self, keys, old_label, new_label):\n",
    "        super().__init__(keys)\n",
    "        self.old_label = old_label\n",
    "        self.new_label = new_label\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \n",
    "        d = dict(data)\n",
    "        \n",
    "        for key in self.keys:\n",
    "            d[key][d[key] == self.old_label] = self.new_label\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "def prepare(\n",
    "            in_dir,\n",
    "            pixdim=(1.2, 1.2,1.0), \n",
    "            spatial_size=[160, 160, 128], \n",
    "            gamma=(0.8,1.25), \n",
    "            roi_center=[100,100,79], \n",
    "            roi_size =[160, 160, 128],\n",
    "            cache=True,\n",
    "            start_file=0,\n",
    "            end_file=None,\n",
    "            train_size=0.8):\n",
    "\n",
    "    set_determinism(seed=0)\n",
    "\n",
    "    flair = natsorted(glob(os.path.join(in_dir, '*/*flair.nii')))[start_file : end_file]\n",
    "    t1 = natsorted(glob(os.path.join(in_dir, '*/*t1.nii')))[start_file : end_file]\n",
    "    t1ce = natsorted(glob(os.path.join(in_dir, '*/*t1ce.nii')))[start_file : end_file]\n",
    "    t2 = natsorted(glob(os.path.join(in_dir, '*/*t2.nii')))[start_file : end_file]\n",
    "    mask = natsorted(glob(os.path.join(in_dir, '*/*seg.nii')))[start_file : end_file]\n",
    "\n",
    "    full_dataset = [{\"vol\": [flair,t1ce,t2,t1], \"seg\": mask} for flair,t1ce,t2,t1,mask in\n",
    "                   zip(flair,t1ce,t2,t1,mask)]\n",
    "    \n",
    "    train_size = int(train_size * len(full_dataset)) \n",
    "    val_size = len(full_dataset) - train_size \n",
    "        \n",
    "        \n",
    "    train_dataset, val_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Set seed for reproducibility\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_transform = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"vol\", \"seg\"]),\n",
    "            EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
    "            EnsureTyped(keys=[\"vol\", \"seg\"]),\n",
    "            \n",
    "            Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
    "            Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
    "            RelabelTransform(keys=[\"seg\"], old_label=4, new_label=3),\n",
    "            \n",
    "            SpatialCropd(keys=[\"vol\",\"seg\"], roi_center=roi_center ,roi_size=roi_size ),\n",
    "\n",
    "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=0),\n",
    "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=1),\n",
    "            RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=2),\n",
    "            \n",
    "            RandGaussianNoised(keys=[\"vol\"], prob=0.6, mean=0.0, std=0.1),\n",
    "            \n",
    "            RandCoarseShuffled(\n",
    "            keys=[\"vol\"],\n",
    "            prob=0.7,\n",
    "            holes=10 ,\n",
    "            spatial_size=(16, 16, 16)\n",
    "            ),\n",
    "            \n",
    "            RandAdjustContrastd(keys=[\"vol\"] ,prob=0.7,gamma=gamma),\n",
    "            \n",
    "            NormalizeIntensityd(keys=\"vol\", nonzero=True, channel_wise=True),\n",
    "            RandScaleIntensityd(keys=\"vol\", factors=0.15, prob=0.7),\n",
    "            RandShiftIntensityd(keys=\"vol\", offsets=0.15, prob=0.7),\n",
    "            ToTensord(keys=[\"vol\", \"seg\"]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    val_transform = Compose(\n",
    "        [\n",
    "                LoadImaged(keys=[\"vol\", \"seg\"]),\n",
    "                EnsureChannelFirstd(keys=[\"vol\", \"seg\"]),\n",
    "                EnsureTyped(keys=[\"vol\", \"seg\"]),\n",
    "                \n",
    "                Orientationd(keys=[\"vol\", \"seg\"], axcodes=\"RAS\"),\n",
    "                Spacingd(keys=[\"vol\", \"seg\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
    "                RelabelTransform(keys=[\"seg\"], old_label=4, new_label=3),\n",
    "                \n",
    "                SpatialCropd(keys=[\"vol\",\"seg\"], roi_center=roi_center ,roi_size=roi_size ),\n",
    "\n",
    "                RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=0),\n",
    "                RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=1),\n",
    "                RandFlipd(keys=[\"vol\", \"seg\"], prob=0.5, spatial_axis=2),\n",
    "                \n",
    "                RandGaussianNoised(keys=[\"vol\"], prob=0.6, mean=0.0, std=0.1),\n",
    "                \n",
    "                RandCoarseShuffled(\n",
    "                keys=[\"vol\"],\n",
    "                prob=0.7,\n",
    "                holes=10 ,\n",
    "                spatial_size=(16, 16, 16)\n",
    "                ),\n",
    "                \n",
    "            NormalizeIntensityd(keys=\"vol\", nonzero=True, channel_wise=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    if cache:\n",
    "        train_ds = CacheDataset(\n",
    "            data=train_dataset,\n",
    "            transform=train_transform,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "\n",
    "        val_ds = CacheDataset(\n",
    "            data=val_dataset,\n",
    "            transform=val_transform,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "\n",
    "        return (\n",
    "    DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0),\n",
    "    DataLoader(val_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    ")\n",
    "\n",
    "\n",
    "    else:\n",
    "        train_ds = Dataset(\n",
    "                    data=train_dataset,\n",
    "                    transform=train_transform,\n",
    "                    cache_rate=1.0,\n",
    "                    num_workers=0,\n",
    "                )\n",
    "        \n",
    "                \n",
    "                \n",
    "        val_ds = Dataset(\n",
    "                    data=val_dataset,\n",
    "                    transform=val_transform,\n",
    "                    cache_rate=1.0,\n",
    "                    num_workers=0,\n",
    "                )\n",
    "\n",
    "        return (\n",
    "    DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0),\n",
    "    DataLoader(val_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = r'G:\\BraTS\\MICCAI_BraTS2020_TestingData'\n",
    "# train_loader , val_loader = prepare(in_dir=train_path,start_file=0,end_file=10)\n",
    "\n",
    "# # Get one batch from the DataLoader\n",
    "# val_batch = next(iter(val_loader))  # assuming val_loader is your DataLoader\n",
    "# # Pick the first sample from the batch\n",
    "# val_data_example = {k: v[0] for k, v in val_batch.items()}\n",
    "\n",
    "# print(f\"Volume shape: {val_data_example['vol'].shape}\")\n",
    "\n",
    "# for slice_idx in range(20, 100, 3):  # slice index\n",
    "#     plt.figure(\"Volume\", (24, 6))\n",
    "#     for ch in range(4):  # channels: flair, t1ce, t2, t1\n",
    "#         plt.subplot(1, 4, ch + 1)\n",
    "#         plt.title(f\"vol channel {ch}\")\n",
    "#         plt.imshow(val_data_example[\"vol\"][ch, :, :, slice_idx].detach().cpu(), cmap=\"gray\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Visualize the segmentation mask\n",
    "#     seg = val_data_example['seg'].detach().cpu()\n",
    "\n",
    "#     print(np.unique(seg))  # Optional: check unique values\n",
    "#     print(f\"Segmentation shape: {seg.shape}\")  # (1, H, W, D)\n",
    "\n",
    "#     plt.figure(\"Segmentation\", (6, 6))\n",
    "#     plt.title(f\"segmentation (slice {slice_idx})\")\n",
    "#     plt.imshow(seg[0, :, :, slice_idx], cmap=\"viridis\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first\n",
    "import matplotlib.pyplot as plt\n",
    "def show_patient(tran_loader,val_loader,SLICE_NUMBER=1 ,train:bool=True,val:bool=False):\n",
    "    \n",
    "    view_train_patient = first(tran_loader)\n",
    "    view_val_patient = first(val_loader)\n",
    "    \n",
    "    if train:\n",
    "        \n",
    "        # Create figure with proper size\n",
    "        plt.figure(figsize=(12, 6))  \n",
    "\n",
    "        # First subplot (Original Image)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'vol {SLICE_NUMBER}')\n",
    "\n",
    "        view_train_patient_image = np.array(view_train_patient['vol'][0, 0, :, :, SLICE_NUMBER], dtype=np.float16 )\n",
    "        plt.imshow(view_train_patient_image, cmap=\"gray\")\n",
    "\n",
    "        # Second subplot (Test Image)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'seg {SLICE_NUMBER}')\n",
    "        view_train_patient_label = np.array(view_train_patient['seg'][0, 0, :, :, SLICE_NUMBER], dtype=np.float16)  \n",
    "        plt.imshow(view_train_patient_label, cmap=\"gray\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    if val:\n",
    "        \n",
    "        # Create figure with proper size\n",
    "        plt.figure(figsize=(12, 6))  \n",
    "\n",
    "        # First subplot (Original Image)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'vol {SLICE_NUMBER}')\n",
    "\n",
    "        view_val_patient = np.array(view_val_patient['vol'][0, 0, :, :, SLICE_NUMBER], dtype=np.float16)  \n",
    "        plt.imshow(view_val_patient, cmap=\"gray\")\n",
    "\n",
    "        # Second subplot (Test Image)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'seg {SLICE_NUMBER}')\n",
    "        view_val_patient = np.array(view_val_patient['seg'][0, 0, :, :, SLICE_NUMBER], dtype=np.float16)  \n",
    "        plt.imshow(view_val_patient, cmap=\"gray\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the transform to a single sample\n",
    "# sample = transforms(train_dataset[0])\n",
    "# print(f\"Volume shape: {sample['vol'].shape}\")\n",
    "# print(f\"Segmentation shape: {sample['seg'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        skip = x \n",
    "        x = self.pool(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv3d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        return x \n",
    "\n",
    "\n",
    "class UNetBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.dropout = nn.Dropout3d(0.3)  # %30 dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UnetModel, self).__init__()\n",
    "\n",
    "        self.encoder1 = UNetEncoder(4, 32)    \n",
    "        self.encoder2 = UNetEncoder(32, 64)   \n",
    "        self.encoder3 = UNetEncoder(64, 128)  \n",
    "        self.encoder4 = UNetEncoder(128, 256) \n",
    "\n",
    "        self.bottleneck = UNetBottleneck(256, 512) \n",
    "\n",
    "        self.decoder1 = UNetDecoder(512, 256)\n",
    "        self.decoder2 = UNetDecoder(256, 128)\n",
    "        self.decoder3 = UNetDecoder(128, 64)\n",
    "        self.decoder4 = UNetDecoder(64, 32)\n",
    "\n",
    "        self.final_conv = nn.Conv3d(32, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, skip1 = self.encoder1(x)\n",
    "        x, skip2 = self.encoder2(x)\n",
    "        x, skip3 = self.encoder3(x)\n",
    "        x, skip4 = self.encoder4(x) \n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoder1(x, skip4)\n",
    "        x = self.decoder2(x, skip3)\n",
    "        x = self.decoder3(x, skip2)\n",
    "        x = self.decoder4(x, skip1)\n",
    "        \n",
    "        x = self.final_conv(x)  \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetModel(4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, smooth=1e-6):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def dice_loss(self, pred, target):\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        intersection = (pred * target).sum(dim=(2, 3, 4))\n",
    "        union = pred.sum(dim=(2, 3, 4)) + target.sum(dim=(2, 3, 4))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        ce = nn.CrossEntropyLoss()(pred, target.argmax(dim=1))  # [B, 128, 128, 128]\n",
    "        return self.alpha * dice + (1 - self.alpha) * ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Bhavesh\\AppData\\Local\\Temp\\ipykernel_4428\\676006854.py:16: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  model_train_val_metric_path=\"D:\\web dev backup\\Pytorch\\Brain_tumor_metric\",\n",
      "C:\\Users\\Bhavesh\\AppData\\Local\\Temp\\ipykernel_4428\\676006854.py:29: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  model_concat_train_val_metric_path = \"D:\\web dev backup\\Pytorch\\Brain_tumor_metric_concat\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    " \n",
    "# Dice Score function\n",
    "def dice_score(pred, target, smooth=1e-6):\n",
    "    pred = torch.softmax(pred, dim=1)  # [B, 4, 128, 128, 128]\n",
    "    intersection = (pred * target).sum(dim=(2, 3, 4))\n",
    "    union = pred.sum(dim=(2, 3, 4)) + target.sum(dim=(2, 3, 4))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "def model_train(model, val_data, train_data,\n",
    "                model_train_val_metric_path=\"D:\\web dev backup\\Pytorch\\Brain_tumor_metric\",\n",
    "                epochs=20, checkpoint_path=\"Brain_tumor/tumor_unet_model_v2.pth\",\n",
    "                i=1):\n",
    "   \n",
    "    save_loss_train = []\n",
    "    save_loss_val = []\n",
    "    save_metric_train = []\n",
    "    save_metric_val = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    model_concat_train_val_metric_path = \"D:\\web dev backup\\Pytorch\\Brain_tumor_metric_concat\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using DataParallel for multi-GPU!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = CombinedLoss()  # Assuming CombinedLoss is defined somewhere\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "    # **Load previous checkpoint if available**\n",
    "    \n",
    "    best_dice = 0.0  # remove it for already epoched dataset\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading previous checkpoint from {checkpoint_path}...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "        # Load the best Dice score if available\n",
    "        if 'best_dice' in checkpoint and checkpoint['best_dice'] is not None:\n",
    "            best_dice = checkpoint['best_dice']\n",
    "            print(f\"Loaded best Dice score from checkpoint: {best_dice:.4f}\")\n",
    "        else:\n",
    "            best_dice = 0.0\n",
    "            print(\"No best Dice score found in checkpoint. Using default value of 0.0.\")\n",
    "\n",
    "        # Load model state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model state loaded successfully!\")\n",
    "\n",
    "        # Load optimizer state (add this)\n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"Optimizer state loaded successfully!\")\n",
    "        else:\n",
    "          print(\"No optimizer state found in checkpoint. Starting with a new optimizer.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5  # Early stopping\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # **Training Phase**\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_dice = 0\n",
    "        train_loader = tqdm(train_data, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "\n",
    "\n",
    "            images, labels = batch_data['vol'], batch_data['seg']\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # [B, 4, 128, 128, 128]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_dice += dice_score(outputs, labels).item()\n",
    "            train_loader.set_postfix({'train_loss': train_loss / (train_loader.n + 1)})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_data)\n",
    "        avg_train_dice = train_dice / len(train_data)\n",
    "        \n",
    "        save_loss_train.append(avg_train_loss)\n",
    "        save_metric_train.append(avg_train_dice)\n",
    "        \n",
    "        file_path_train_loss = os.path.join(model_concat_train_val_metric_path,f'loss_train_{i}.npy')\n",
    "        file_path_train_metric = os.path.join(model_concat_train_val_metric_path,f'metric_train_{i}.npy')\n",
    "        \n",
    "        if os.path.exists(file_path_train_loss) and os.path.exists(file_path_train_metric):\n",
    "            \n",
    "            # Load existing data\n",
    "            train_loss_data = np.load(file_path_train_loss)\n",
    "            train_metric_data = np.load(file_path_train_metric)\n",
    "            \n",
    "            # Append along axis=0\n",
    "            train_loss_data = np.concatenate((train_loss_data, save_loss_train), axis=0)\n",
    "            train_metric_data = np.concatenate((train_metric_data, save_metric_train), axis=0)\n",
    "            \n",
    "        else:\n",
    "        # If file doesn't exist, just use new_data as initial data\n",
    "            train_loss_data = save_loss_train\n",
    "            train_metric_data = save_metric_train\n",
    "\n",
    "\n",
    "        np.save(file_path_train_loss, train_loss_data)\n",
    "        np.save(file_path_train_metric, train_metric_data)\n",
    "\n",
    "        np.save(os.path.join(model_train_val_metric_path, 'loss_train.npy'), save_loss_train)\n",
    "        np.save(os.path.join(model_train_val_metric_path, 'metric_train.npy'), save_metric_train)\n",
    "\n",
    "        \n",
    "        # **Validation Phase**\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_dice = 0\n",
    "        val_loader = tqdm(val_data, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_dice += dice_score(outputs, labels).item()\n",
    "\n",
    "                val_loader.set_postfix({'val_loss': val_loss / (val_loader.n + 1),\n",
    "                                        'val_dice': val_dice / (val_loader.n + 1)})\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_data)\n",
    "        avg_val_dice = val_dice / len(val_data)\n",
    "        \n",
    "        save_loss_val.append(avg_val_loss)\n",
    "        save_metric_val.append(avg_val_dice)\n",
    "\n",
    "        \n",
    "        file_path_val_loss = os.path.join(model_concat_train_val_metric_path,f'loss_val_{i}.npy')\n",
    "        file_path_val_metric = os.path.join(model_concat_train_val_metric_path,f'metric_val_{i}.npy')\n",
    "        \n",
    "        if os.path.exists(file_path_val_loss) and os.path.exists(file_path_val_metric):\n",
    "            \n",
    "            # Load existing data\n",
    "            val_loss_data = np.load(file_path_val_loss)\n",
    "            val_metric_data = np.load(file_path_val_metric)\n",
    "            \n",
    "            # Append along axis=0\n",
    "            val_loss_data = np.concatenate((val_loss_data, save_loss_val), axis=0)\n",
    "            val_metric_data = np.concatenate((val_metric_data, save_metric_val), axis=0)\n",
    "            \n",
    "        else:\n",
    "        # If file doesn't exist, just use new_data as initial data\n",
    "            val_loss_data = save_loss_val\n",
    "            val_metric_data = save_metric_val\n",
    "\n",
    "\n",
    "        np.save(file_path_val_loss, val_loss_data)\n",
    "        np.save(file_path_val_metric, val_metric_data)\n",
    "\n",
    "        np.save(os.path.join(model_train_val_metric_path, 'loss_val.npy'), save_loss_val)\n",
    "        np.save(os.path.join(model_train_val_metric_path, 'metric_val.npy'), save_metric_val)\n",
    "\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        epoch_duration = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Training Loss: {avg_train_dice:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation Dice: {avg_val_dice:.4f}')\n",
    "        print(f'Time Taken: {epoch_duration:.2f} seconds')\n",
    "        print('-' * 50)\n",
    "\n",
    "        # **Save the best model based on Dice Score**\n",
    "        if avg_val_dice > best_dice:\n",
    "            best_dice = avg_val_dice\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "                    # Save the best model only\n",
    "                    # Save the model along with the best dice score\n",
    "            torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'best_dice': best_dice\n",
    "                        }, \"/content/drive/My Drive/Brain_tumor/unet_model_v10.pth\")\n",
    "\n",
    "            print(f\"Best model saved with Dice: {best_dice:.4f}, Val Loss: {best_val_loss:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 8/8 [00:34<00:00,  4.25s/it]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]\n"
     ]
    }
   ],
   "source": [
    "train_path = r'G:\\BraTS\\MICCAI_BraTS2020_TestingData'\n",
    "train_loader , val_loader = prepare(in_dir=train_path,start_file=0,end_file=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)\n",
    "print(next(model.parameters()).device)  # Should print cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "No checkpoint found. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/4 [00:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.51 GiB is allocated by PyTorch, and 52.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBrain_tumor/tumor_unet_model_v41.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mmodel_train\u001b[39m\u001b[34m(model, val_data, train_data, model_train_val_metric_path, epochs, checkpoint_path, i)\u001b[39m\n\u001b[32m     89\u001b[39m images, labels = images.to(device), labels.to(device)\n\u001b[32m     91\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, 4, 128, 128, 128]\u001b[39;00m\n\u001b[32m     93\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     95\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mUnetModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     82\u001b[39m x = \u001b[38;5;28mself\u001b[39m.decoder2(x, skip3)\n\u001b[32m     83\u001b[39m x = \u001b[38;5;28mself\u001b[39m.decoder3(x, skip2)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_conv(x)  \n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mUNetDecoder.forward\u001b[39m\u001b[34m(self, x, skip)\u001b[39m\n\u001b[32m     33\u001b[39m x = torch.cat([x, skip], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.bn1(\u001b[38;5;28mself\u001b[39m.conv1(x)))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2807\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2801\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Apply Batch Normalization for each channel across a batch of data.\u001b[39;00m\n\u001b[32m   2802\u001b[39m \n\u001b[32m   2803\u001b[39m \u001b[33;03mSee :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,\u001b[39;00m\n\u001b[32m   2804\u001b[39m \u001b[33;03m:class:`~torch.nn.BatchNorm3d` for details.\u001b[39;00m\n\u001b[32m   2805\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias):\n\u001b[32m-> \u001b[39m\u001b[32m2807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2809\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2810\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2811\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2812\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2815\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2816\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2817\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\overrides.py:1742\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     warnings.warn(\n\u001b[32m   1735\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1736\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1737\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1738\u001b[39m     )\n\u001b[32m   1740\u001b[39m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[32m   1741\u001b[39m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m result = \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\monai\\data\\meta_tensor.py:282\u001b[39m, in \u001b[36mMetaTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    281\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:1648\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1647\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1650\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.51 GiB is allocated by PyTorch, and 52.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_train(model, val_data= val_loader, train_data= train_loader, epochs=10,checkpoint_path=\"Brain_tumor/tumor_unet_model_v41.pth\",i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"D:\\web dev backup\\Pytorch\\Brain_tumor_metric\"\n",
    "train_loss = np.load(os.path.join(model_dir, 'loss_train.npy'))\n",
    "train_metric = np.load(os.path.join(model_dir, 'metric_train.npy'))\n",
    "val_loss = np.load(os.path.join(model_dir, 'loss_val.npy'))\n",
    "val_metric = np.load(os.path.join(model_dir, 'metric_val.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"Results 25 june\", (12, 12))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Train dice loss\")\n",
    "x = [i + 1 for i in range(len(train_loss))]\n",
    "y = train_loss\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Train metric DICE\")\n",
    "x = [i + 1 for i in range(len(train_metric))]\n",
    "y = train_metric\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title(\"Test dice loss\")\n",
    "x = [i + 1 for i in range(len(val_loss))]\n",
    "y = val_loss\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"Test metric DICE\")\n",
    "x = [i + 1 for i in range(len(val_metric))]\n",
    "y = val_metric\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
