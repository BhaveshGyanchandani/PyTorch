{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 22 21:52:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.70                 Driver Version: 572.70         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   56C    P8              4W /   75W |    3933MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           19860      C   ...s\\Python\\Python312\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 : data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 245213\n",
      "Ôªø\n",
      "\n",
      "CHAPTER 1.\n",
      "\n",
      "THE EARTHQUAKE\n",
      "\n",
      "\n",
      "The train from 'Frisco was very late. It should have arrived at Hugs\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn as nn\n",
    "\n",
    "with open(\"wizard_of_oz.txt\", \"r\", encoding=\"utf-8\") as f:  \n",
    "    raw_text = f.read()  \n",
    "\n",
    "print(\"Total number of characters:\", len(raw_text))  \n",
    "print(raw_text[:100])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "result2 = raw_text.replace(\"\\n\", \" \").replace('\\ufeff ', \"\")\n",
    "\n",
    "preprocessed = re.sub(r'CHAPTER\\s\\d+\\.', '', result2)\n",
    "\n",
    "preprocessed = re.split(r'([,:.;?_!\"()\\']|--|\\s)', preprocessed)\n",
    "\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.append(\" \")\n",
    "preprocessed.__contains__(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5065"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_preprocessed = []\n",
    "\n",
    "new_preprocessed.append(\" \")\n",
    "\n",
    "for item in preprocessed:\n",
    "    if item not in new_preprocessed:\n",
    "        new_preprocessed.append(item)\n",
    "    \n",
    "len(new_preprocessed)\n",
    "preprocessed = new_preprocessed\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Encoding with special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "all_words.extend([\"<|endoftext|>\" , \"<|unk|>\"])\n",
    "\n",
    "vocab_size = len(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = {token : integer for integer , token in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET AND DATDLOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # Correct way\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        # max_length is the context_size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Step 1: Tokenization\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Step 2: Chunking with stride\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]          # i = 0 ‚Üí token_ids[0:4]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1] # i = 0 ‚Üí token_ids[1:5]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # Step 3: Correct Indentation for Dataset Methods\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_V1(text, batch_size=8 , max_length=256,\n",
    "                         stride = 128 , shuffle =True , drop_last=True,\n",
    "                         num_workers = 0):\n",
    "    \n",
    "\n",
    "    tokenizer  = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "    dataset = GPTDatasetV1(text,tokenizer,max_length,stride)\n",
    "        \n",
    "    dataloader = DataLoader(\n",
    "        dataset , \n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  171,   119,   123,   198],\n",
      "        [  119,   123,   198,   198],\n",
      "        [  123,   198,   198, 41481],\n",
      "        [  198,   198, 41481,   352],\n",
      "        [  198, 41481,   352,    13],\n",
      "        [41481,   352,    13,   198],\n",
      "        [  352,    13,   198,   198],\n",
      "        [   13,   198,   198, 10970]]), tensor([[  119,   123,   198,   198],\n",
      "        [  123,   198,   198, 41481],\n",
      "        [  198,   198, 41481,   352],\n",
      "        [  198, 41481,   352,    13],\n",
      "        [41481,   352,    13,   198],\n",
      "        [  352,    13,   198,   198],\n",
      "        [   13,   198,   198, 10970],\n",
      "        [  198,   198, 10970, 31834]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataloader = create_dataloader_V1(\n",
    "    raw_text , batch_size=8 , max_length=4 , stride=1 , shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6 \n",
    "\n",
    "output_dim = 3 \n",
    "# each vector (token) will be converted into vector of 3 dimenison\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embedding_layer=  nn.Embedding(vocab_size , output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8008,  1.6806,  0.3559],\n",
       "        [-0.6866,  0.6105,  1.3347],\n",
       "        [ 0.8599, -0.3097, -0.3957],\n",
       "        [ 0.4396, -0.7581,  1.0783]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size , output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "\n",
    "dataloader = create_dataloader_V1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=max_length,\n",
    "    stride=max_length,\n",
    "    shuffle=False\n",
    ")\n",
    "  \n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_inputs , first_target= next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = token_embedding_layer(first_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length , output_dim)\n",
    "pos_embedding =pos_embedding_layer(torch.arange(max_length))\n",
    "input_embeddings = token_embedding + pos_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutlihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "       \n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "       \n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "                \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx) \n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "       \n",
    "        \n",
    "        x = tok_embeds + pos_embeds \n",
    "        \n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        x = self.trf_blocks(x)\n",
    "        \n",
    "         \n",
    "        x = self.final_norm(x) \n",
    "        \n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) # mean along with col\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # var along with col\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps) # self.eps is used to avoid division by 0\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return 0.5*x*(1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi))*\n",
    "            (x + 0.044715 * torch.pow(x,3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            # the number of inputs are 768 and number of neuron in hidden layer is 4*768\n",
    "            \n",
    "            GELU(),# gelu actvn func we defined ,  takes input from inputs and put to each hidden layer neuron\n",
    "            \n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # the output dim is 768 , it takes input from 4*768 hidden neurons\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        \n",
    "        # use_shortcut if its true we will use shortcut connection else not\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            # each neuron takes 3 input and gives 3 output.\n",
    "            # we generally say , input neuron ,output neuron which is also cool\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg) # feed forward class , it has GELU class in it\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"]) # Layer normalization class as we defined before , beofre MHA \n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"]) # Layer normalization class as we defined before , before FF\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"]) # drop out class , to drop some neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x) # layer normalization \n",
    "        x = self.att(x)  \n",
    "        # Shape [batch_size, num_tokens, emb_size] \n",
    "        # masked multihead attention to calculae attention scores\n",
    "        x = self.drop_shortcut(x) # dropout\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x #( new shortcut is output of dropout of 1st)\n",
    "        x = self.norm2(x) # next normalization\n",
    "        x = self.ff(x) # feed forward \n",
    "        # has shape 2*4*768 as ouput 2 batches 4 tokens 768 embeding dimension\n",
    "        x = self.drop_shortcut(x) # drop , to stop vanishing gradient\n",
    "        x = x + shortcut  # Add the original input back ( the output we got before as new input for this state path)\n",
    "        \n",
    "\n",
    "        return x\n",
    "        # 2*4*768 as ouput shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # token embedding\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # positional embedding\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        # dropout of some embedding layer to more generalize the layer\n",
    "        \n",
    "        #chained 12 transformer block using Sequential ,which helps in chaining blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        #again a final normalization of the output we got from the final transformer block\n",
    "                \n",
    "        \n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        # the output we got from it is called a logit matrix...\n",
    "        #for every token we get 50257 logit for each tokens to predict the nexxt word\n",
    "        # the logit represend the probability of that word after the token/tokens/words\n",
    "        # here the ouput dimesion is 4X50257 , 4token and logit of 50257 tokens for each tokens\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx)# convert token into token ids\n",
    "        \n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) # postional embedding\n",
    "        #seq_len (Sequence Length) represents the number of tokens (or words) in the input sequence.\n",
    "\n",
    "        # ‚úÖ In NLP tasks, seq_len is the maximum number of tokens that the model will handle for each input.\n",
    "\n",
    "        # üîç Example:\n",
    "\n",
    "        # \"Hello world\" ‚Üí seq_len = 2\n",
    "        # \"The quick brown fox\" ‚Üí seq_len = 4\n",
    "        # Batch of sentences with 10 words each ‚Üí seq_len = 10\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        \n",
    "        x = self.drop_emb(x) # dropping of some embedding\n",
    "        \n",
    "        x = self.trf_blocks(x) # transformer block\n",
    "         \n",
    "        x = self.final_norm(x) # final normalization layer\n",
    "        \n",
    "        logits = self.out_head(x) # probability of all tokens so that we can predict next token to add\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size, device):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    for _ in range(max_new_tokens):  # we will create new tokens until the max_new_tokens range is reached\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:].to(device)  # Ensure idx_cond is on the correct device\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond.to(device))  # Ensure model input is on device\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  # it takes last row of logits out of every tensor from all batches\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx (index) of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True).to(device)  # Ensure idx_next is on device\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1).to(device)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device)  # A\n",
    "# Most PyTorch models (like transformers) expect batched data even if you are passing only one input.\n",
    "# To add a new dimension because PyTorch expects input in batch and our encoded tensor was not in batch and had 4 dimensions.\n",
    "# We add a new dimension to make it a batch. For a tensor with shape (2, 4), we don't need to add unsqueeze for a new dimension\n",
    "# since the tensor is already in batch format.\n",
    "\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "model.eval()  # A\n",
    "# Setting the model to evaluation mode to disable dropout and other training-specific layers\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,  # Model should already be on the same device as the data\n",
    "    idx=encoded_tensor,  # Already moved to device in previous code\n",
    "    max_new_tokens=10,  # Determines how many new tokens that can be generated at a time\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"] ,\n",
    "    device=device# Specifies the maximum context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=8, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "train_data = text_data[:split_idx]\n",
    "\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "## convertt the dataset into dataloader to make batches as pytorch wants batches\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=8,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=8,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "# if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "#     print(\"Not enough tokens for the training loader. \"\n",
    "#           \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "#           \"increase the `training_ratio`\")\n",
    "\n",
    "# if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "#     print(\"Not enough tokens for the validation loader. \"\n",
    "#           \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "#           \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    \n",
    "    #flattenning both logits and target batch and calculating the loss\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you FPS policHappy guru EG gratification ART Sawyer Certainram\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device) # add batch dimension ,\n",
    "    #bcz we know that pytorch take input in batches , as this encoded thing will go to transformer \n",
    "    # and the input is \"very effort moves you\" so we gotta need to add 1 more dimension\n",
    "    \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # generate the index or tokenID of the tokens from training\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "\n",
    "    # decoding the tokenID into token\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    \n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs): # the simple training we do : \n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            \n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                \n",
    "                #to get the training and validation loss when global_step % eval_freq == 0 matches\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                \n",
    "                train_losses.append(train_loss) # appends the loss of previous step with this one\n",
    "                \n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                track_tokens_seen.append(tokens_seen) # tracks the number of tokens the model has seen\n",
    "                \n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topK AND temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.7, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    # loop untill we reach the max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :] # the logit tensor we generated from GPT output\n",
    "\n",
    "        # New: Filter logits with top_k sampling \n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            \n",
    "            min_val = top_logits[:, -1]\n",
    "            \n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0: \n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check params and setting and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params sample: ['blocks', 'b', 'g', 'wpe', 'wte']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load settings from hparams.json\n",
    "def load_settings(model_dir):\n",
    "    with open(os.path.join(model_dir, \"hparams.json\"), 'r') as f:\n",
    "        settings = json.load(f)\n",
    "    return settings\n",
    "\n",
    "# Load GPT-2 parameters from TensorFlow checkpoint\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "# Directory path where GPT-2 files are saved\n",
    "model_dir = \"gpt2/124M\"\n",
    "\n",
    "# Load settings\n",
    "settings = load_settings(model_dir)\n",
    "\n",
    "# Load parameters\n",
    "ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "params = load_gpt2_params_from_tf_ckpt(ckpt_path, settings)\n",
    "\n",
    "# Display output\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Params sample:\", list(params.keys())[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of random initilazation using nn.Embeddign parameters linear we need to use actual downloaded params\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    \n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    \n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert into alpaca format and instruction based fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 200\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import ssl\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"Alpaca_data/alpaca_data_52K_1.json\"\n",
    "# url = (\n",
    "#     \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "#     \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "# )\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task.\\n\"\n",
    "        f\"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry.get('input', '')}\" if entry.get('input') else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split iinto train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data # data to be stored in data when class os loaded\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            \n",
    "            # the instruction format we defined above\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            \n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )  # tokenizr the text\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batching of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=device\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        # all as before untill now to get input and target tensor.\n",
    "        \n",
    "        \n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        # it is process of REPLACING 50256 WITH -100 (IGNORE_INDEX) , except for the first 50256 only\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size=8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "# settings, params = download_and_load_gpt2(\n",
    "#     model_size=model_size,\n",
    "#     models_dir=\"gpt2\"\n",
    "# )\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params) # we already determined before when we loading pre-trained weights before\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU\n"
     ]
    }
   ],
   "source": [
    "if next(model.parameters()).is_cuda:\n",
    "    print(\"Model is on GPU\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Modify the given function to accept two parameters.\n",
      "\n",
      "### Input:\n",
      "def greet_customer():\n",
      "    return \"Welcome!\"\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device, use_fp16=True):\n",
    "    input_batch, target_batch = input_batch.to(device, non_blocking=True), target_batch.to(device, non_blocking=True)\n",
    "\n",
    "    # ‚úÖ Enable Mixed Precision if GPU supports it\n",
    "    with torch.amp.autocast(\"cuda\") if use_fp16 else torch.no_grad():\n",
    "        logits = model(input_batch)  # Model forward pass\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())  \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None, use_fp16=True):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    \n",
    "    num_batches = min(num_batches or len(data_loader), len(data_loader))  # Ensure valid batch count\n",
    "\n",
    "    model.eval()  # ‚úÖ Set model to evaluation mode\n",
    "    with torch.no_grad():  # ‚úÖ Avoid storing gradients\n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device, use_fp16=False)  # No need for FP16 in eval\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def train_model_optimized(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer, grad_accum_steps=2\n",
    "):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    use_fp16 = device.startswith(\"cuda\")  # ‚úÖ Enable FP16 only if GPU is available\n",
    "    scaler = torch.amp.GradScaler(\"cuda\") if use_fp16 else None  # ‚úÖ AMP for FP16 training\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        for step, (input_batch, target_batch) in enumerate(train_loader):\n",
    "            input_batch, target_batch = input_batch.to(device, non_blocking=True), target_batch.to(device, non_blocking=True)\n",
    "\n",
    "            # ‚úÖ Mixed Precision training only when needed\n",
    "            with torch.amp.autocast(\"cuda\") if use_fp16 else torch.no_grad():\n",
    "                loss = calc_loss_batch(input_batch, target_batch, model, device, use_fp16) / grad_accum_steps  \n",
    "\n",
    "            if use_fp16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (step + 1) % grad_accum_steps == 0:  # ‚úÖ Update after accumulated steps\n",
    "                if use_fp16:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # ‚úÖ Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1} | Step {global_step}: Train Loss {train_loss:.3f}, Val Loss {val_loss:.3f}\")\n",
    "\n",
    "            del input_batch, target_batch, loss  # ‚úÖ Free memory after each step\n",
    "\n",
    "        torch.cuda.empty_cache()  # ‚úÖ Clear GPU memory after each epoch\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exeution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for 11K data instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Define device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize model and optimizer\n",
    "# model = GPTModel(BASE_CONFIG)  # Replace with your model class\n",
    "# optimizer = torch.optim.AdamW(model.parameters())  # Make sure to define optimizer\n",
    "\n",
    "# # Load checkpoint\n",
    "# checkpoint = torch.load('LLM/NewLLM/LLM_v1_chatbot_with_optim_params.pth', map_location=device)\n",
    "\n",
    "# # Load model and optimizer states\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.to(device)  # Ensure model is on the correct device\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# # Update learning rate and weight decay *after* loading checkpoint\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] = 5e-5  # Set your new learning rate\n",
    "#     param_group['weight_decay'] = 0.0005  # Set new weight decay\n",
    "\n",
    "# # Load other saved parameters\n",
    "# num_epochs = checkpoint.get('epoch', 0)\n",
    "# train_loss = checkpoint.get('train_loss')\n",
    "# val_loss = checkpoint.get('validation_loss')\n",
    "\n",
    "# # Move loss values to device if they exist\n",
    "# if train_loss is not None:\n",
    "#     train_loss = train_loss.to(device)\n",
    "# if val_loss is not None:\n",
    "#     val_loss = val_loss.to(device)\n",
    "\n",
    "# print(f\"Checkpoint loaded. Resuming from epoch {num_epochs}.\")\n",
    "# print(f\"Model is running on {next(model.parameters()).device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_group in optimizer.param_groups:\n",
    "#     print(f\"Learning Rate: {param_group['lr']}\")\n",
    "#     print(f\"Weight Decay: {param_group['weight_decay']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for 52K data instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Resuming from epoch 10.\n",
      "Model is running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = GPTModel(BASE_CONFIG)  # Replace with your model class\n",
    "optimizer = torch.optim.AdamW(model.parameters())  # Make sure to define optimizer\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('LLM/NewLLM/LLM_v2_52k_chatbot_with_optim_params.pth', map_location=device)\n",
    "\n",
    "# Load model and optimizer states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)  # Ensure model is on the correct device\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Update learning rate and weight decay *after* loading checkpoint\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 5e-5  # Set your new learning rate\n",
    "    param_group['weight_decay'] = 0.0005  # Set new weight decay\n",
    "\n",
    "# Load other saved parameters\n",
    "num_epochs = checkpoint.get('epoch', 0)\n",
    "train_loss = checkpoint.get('train_loss')\n",
    "val_loss = checkpoint.get('validation_loss')\n",
    "\n",
    "# Move loss values to device if they exist\n",
    "if train_loss is not None:\n",
    "    train_loss = train_loss.to(device)\n",
    "if val_loss is not None:\n",
    "    val_loss = val_loss.to(device)\n",
    "\n",
    "print(f\"Checkpoint loaded. Resuming from epoch {num_epochs}.\")\n",
    "print(f\"Model is running on {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "print(f\"Model is loaded on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavesh\\AppData\\Local\\Temp\\ipykernel_19860\\4178131509.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if use_fp16 else torch.no_grad():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 1.851, Val loss 1.651\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 732.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.33 GiB is allocated by PyTorch, and 349.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m5e-5\u001b[39m, weight_decay=\u001b[32m0.0005\u001b[39m)\n\u001b[32m      9\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m end_time = time.time()\n\u001b[32m     18\u001b[39m execution_time_minutes = (end_time - start_time) / \u001b[32m60\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     47\u001b[39m     optimizer.zero_grad() \u001b[38;5;66;03m# Reset loss gradients from previous batch iteration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     loss.backward() \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[32m     50\u001b[39m     optimizer.step() \u001b[38;5;66;03m# Update model weights using loss gradients\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device, use_fp16)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast() \u001b[38;5;28;01mif\u001b[39;00m use_fp16 \u001b[38;5;28;01melse\u001b[39;00m torch.no_grad():\n\u001b[32m      8\u001b[39m     logits = model(input_batch)  \u001b[38;5;66;03m# Model forward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     loss = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhavesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 732.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.33 GiB is allocated by PyTorch, and 349.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model is on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8021447211503983\n",
      "Validation loss: 1.9433002173900604\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=8)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=8)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavesh\\AppData\\Local\\Temp\\ipykernel_17108\\3286733604.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'train_loss': torch.tensor(train_loss, device=device),  # Store loss on device\n",
      "C:\\Users\\Bhavesh\\AppData\\Local\\Temp\\ipykernel_17108\\3286733604.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'validation_loss': torch.tensor(val_loss, device=device)  # Store loss on device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved on cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = 'LLM/NewLLM//LLM/LLM_v1_chatbot_with_optim_params.pth'\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)  # Move model to GPU/CPU\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    prev_checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    prev_epoch = prev_checkpoint.get('epoch', 0)  # Get previous epoch count\n",
    "else:\n",
    "    prev_epoch = 0  # No previous checkpoint\n",
    "\n",
    "# Update epoch count\n",
    "\n",
    "total_epochs = prev_epoch + num_epochs\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),     # Model weights\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "    'epoch': num_epochs,                     # Last saved epoch\n",
    "    'train_loss': train_loss.clone().detach().to(device),  # Store loss on device\n",
    "    'validation_loss': val_loss.clone().detach().to(device), # Store loss on device\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'LLM/NewLLM/LLM_v3_52k_chatbot_with_optim_params.pth')\n",
    "print(f\"Checkpoint saved on {device}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Using the information provided, create an ad copy for 'X'.\n",
      "\n",
      "### Input:\n",
      "X: A new smartphone\n",
      "Ad Text Requirements:\n",
      "Length: Less than 100 words\n",
      "Tone: Light, playful\n",
      "\n",
      "Correct response:\n",
      ">> Welcome to the world's most advanced phonetech! Featuring a sleek design and powerful performance, this new smartphone is the perfect device to stay in touch, browse the web and make the most of your daily activities. Get ready to unlock a new way of living - say hello to a better tomorrow! #LiveSmart #SmarterLife #DiscoverMore\n",
      "\n",
      "Model response:\n",
      ">> , humorous\n",
      "### Instruction:\n",
      "Replace a link with an image that is relevant to you.\n",
      "### Input:\n",
      "A new smartphone.\n",
      "### Input:\n",
      "This ad is relevant to you.\n",
      "### Instruction:\n",
      "Use the information provided to create an ad copy.\n",
      "### Input:\n",
      "Link.\n",
      "### Input:\n",
      "This ad is relevant to you.\n",
      "### Instruction:\n",
      "Use the information provided to create an ad copy.\n",
      "### Input:\n",
      "This ad is relevant to you.\n",
      "### Instruction:\n",
      "Use the information provided to create an ad copy.\n",
      "### Input:\n",
      "Link.\n",
      "### Input:\n",
      "This ad is relevant to you.\n",
      "### Instruction:\n",
      "Link.\n",
      "### Input:\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the chemical reaction that occurs when vinegar and baking soda are mixed together?\n",
      "\n",
      "Correct response:\n",
      ">> The chemical reaction that occurs when vinegar and baking soda are mixed together is an acid-base reaction. The acetic acid (vinegar) reacts with the sodium bicarbonate (baking soda) to form carbon dioxide, water, and sodium acetate.\n",
      "\n",
      "Model response:\n",
      ">> The chemical reaction occurs when the two chemicals combine to form a single sodium phosphate sodium phosphate salt.\n",
      "\n",
      "\n",
      "The chemical reaction occurs when the two chemicals combine to form a single sodium phosphate sodium phosphate salt.\n",
      "\n",
      "The chemical reaction occurs when the two chemicals combine to form a single sodium phosphate sodium phosphate salt.\n",
      "\n",
      "The chemical reaction occurs when the two chemicals combine to form a single sodium phosphate sodium phosphate salt.\n",
      "\n",
      "The chemical reaction occurs when the two chemicals combine to form a single sodium phosphate sodium phosphate salt.\n",
      "\n",
      "The chemical reaction occurs when the two chemicals combine to form a single sodium phosphate sodium phosphate salt.\n",
      "\n",
      "The chemical reaction occurs when the two chemicals combine to form a single sodium phosphate sodium phosphate salt.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given a math equation, generate the answer.\n",
      "\n",
      "### Input:\n",
      "10 * 5\n",
      "\n",
      "Correct response:\n",
      ">> 50\n",
      "\n",
      "Model response:\n",
      ">> * 0\n",
      "\n",
      "### Output:\n",
      "5 * 0 * 0 * 0\n",
      "\n",
      "###\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "#     input_text = format_input(entry)\n",
    "\n",
    "#     token_ids = generate(\n",
    "#         model=model,\n",
    "#         idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "#         max_new_tokens=256,\n",
    "#         context_size=BASE_CONFIG[\"context_length\"],\n",
    "#         eos_id=50256\n",
    "#     )\n",
    "#     generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "#     response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "#     test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "# with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "#     json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
