{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Dorothy and the Wizard in Oz\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no rest\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "Device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä¢', '‚Ñ¢', '\\ufeff']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars) , print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will use tokenizer here for chars ......\n",
    "#tokenizer consist of a encoder and an decoder , it actually gonna convert eaach element of chars into an integer.....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 62, 69, 69, 72]\n",
      "a^eeh\n",
      "a^eeh\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "string_to_int = {ch:i for i , ch in enumerate(chars)}\n",
    "# int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [string_to_int[c] for c in s ]\n",
    "# decode = lambda l : ''.join([int_to_string[i] for i in l ])\n",
    "int_to_string = {i: chr(32 + (i % 94)) for i in range(500)}\n",
    "decode = lambda l: ''.join([int_to_string.get(i, '?') for i in l])\n",
    "\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))\n",
    "print(decode([65, 62, 69, 69, 72]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91, 48, 65, 62, 1, 44, 75, 72, 67, 62, 60, 77, 1, 35, 78, 77, 62, 71, 59, 62, 75, 64, 1, 62, 30, 72, 72, 68, 1, 72, 63, 1, 32, 72, 75, 72, 77, 65, 82, 1, 58, 71, 61, 1, 77, 65, 62, 1, 51, 66, 83, 58, 75, 61, 1, 66, 71, 1, 43, 83, 0, 1, 1, 1, 1, 0, 48, 65, 66, 76, 1, 62, 59, 72, 72, 68, 1, 66, 76, 1, 63, 72, 75, 1, 77, 65, 62, 1, 78, 76, 62, 1, 72, 63, 1, 58, 71, 82, 72, 71]\n"
     ]
    }
   ],
   "source": [
    "print(encode(text)[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([91, 48, 65, 62,  1, 44, 75, 72, 67, 62, 60, 77,  1, 35, 78, 77, 62, 71,\n",
      "        59, 62, 75, 64,  1, 62, 30, 72, 72, 68,  1, 72, 63,  1, 32, 72, 75, 72,\n",
      "        77, 65, 82,  1, 58, 71, 61,  1, 77, 65, 62,  1, 51, 66, 83, 58, 75, 61,\n",
      "         1, 66, 71,  1, 43, 83,  0,  1,  1,  1,  1,  0, 48, 65, 66, 76,  1, 62,\n",
      "        59, 72, 72, 68,  1, 66, 76,  1, 63, 72, 75,  1, 77, 65, 62,  1, 78, 76,\n",
      "        62,  1, 72, 63,  1, 58, 71, 82, 72, 71])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 1-> [......] has 8 elements\n",
    "# block 2-> [......] has 8 elements\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# block n-> [......] has 8 elements\n",
    "\n",
    "# there are n number of blocks with size 8 , i.e the batch_size = n , block_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([91]) target is  tensor(91)\n",
      "when input is  tensor([91, 48]) target is  tensor(48)\n",
      "when input is  tensor([91, 48, 65]) target is  tensor(65)\n",
      "when input is  tensor([91, 48, 65, 62]) target is  tensor(62)\n",
      "when input is  tensor([91, 48, 65, 62,  1]) target is  tensor(1)\n",
      "when input is  tensor([91, 48, 65, 62,  1, 44]) target is  tensor(44)\n",
      "when input is  tensor([91, 48, 65, 62,  1, 44, 75]) target is  tensor(75)\n",
      "when input is  tensor([91, 48, 65, 62,  1, 44, 75, 72]) target is  tensor(72)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size  = 4\n",
    "\n",
    "n=int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "x=train_data[:block_size]\n",
    "y=train_data[:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"when input is \" ,context,'target is ',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: tensor([154706, 173511, 161096, 118434])\n",
      "Inputs  : tensor([[75,  1, 61, 75, 58, 64, 72, 71],\n",
      "        [70, 76,  1, 77, 65, 58, 77,  1],\n",
      "        [ 1, 77, 65, 58, 77,  1, 65, 58],\n",
      "        [58, 71, 72, 77, 65, 62, 75,  1]])\n",
      "Targets : tensor([[ 1, 61, 75, 58, 64, 72, 71,  1],\n",
      "        [76,  1, 77, 65, 58, 77,  1, 80],\n",
      "        [77, 65, 58, 77,  1, 65, 58, 61],\n",
      "        [71, 72, 77, 65, 62, 75,  1, 72]])\n"
     ]
    }
   ],
   "source": [
    "# Sample data (example for demonstration)\n",
    "# data = torch.arange(100)\n",
    "\n",
    "# Train-Validation Split\n",
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    #ix = index\n",
    "    \n",
    "    print(f\"indices: {ix}\")\n",
    "    \n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])  # Shifted by 1 for next token prediction\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Fetch data\n",
    "x, y = get_batch('train')\n",
    "print(f\"Inputs  : {x}\")\n",
    "print(f\"Targets : {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logits are basically bunch of floating point numbers which are unnormalized\n",
    "#Logits are the raw, unnormalized output values produced by the last layer of a neural network before passing through an activation function like softmax or sigmoid.\n",
    "#Why Use Logits?\n",
    "\n",
    "Logits simplify the computation of loss functions like:\n",
    "\n",
    "nn.CrossEntropyLoss() (automatically applies softmax inside).\n",
    " nn.BCEWithLogitsLoss() (automatically applies sigmoid inside).\n",
    " Using logits directly in these loss functions is numerically more stable than applying softmax or sigmoid separately.\n",
    "\n",
    " In short, logits are just the network‚Äôs \"raw predictions\" before being converted into meaningful probabilities. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([ 2.0000,  1.5000, -0.5000])\n",
      "Probabilities: tensor([0.5922, 0.3592, 0.0486])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Sample logits output\n",
    "logits = torch.tensor([2.0, 1.5, -0.5])\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "probabilities = nn.Softmax(dim=0)(logits) # logits are basically distribution of what we wanna predict\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Embedding in NLP\n",
    "A token embedding is a numerical representation of a text token (like a word, subword, or character) in a continuous vector space. It‚Äôs a key concept in Natural Language Processing (NLP) that allows models like Transformers, BERT, and GPT to understand text data efficiently.\n",
    "\n",
    "How Does Token Embedding Work?\n",
    "Tokenization:\n",
    "\n",
    "First, text is split into smaller units called tokens.\n",
    "These tokens can be words, subwords, or characters depending on the tokenizer used.\n",
    "Example:\n",
    "css\n",
    "Copy\n",
    "Edit\n",
    "\"I love PyTorch\" ‚Üí [\"I\", \"love\", \"Py\", \"##Torch\"]\n",
    "Embedding Layer:\n",
    "Each token is mapped to a high-dimensional vector through an embedding matrix.\n",
    "\n",
    "Suppose the vocabulary size is 10,000 and each token gets embedded into a 768-dimensional vector.\n",
    "The embedding matrix will have a shape of (10000, 768).\n",
    "Vector Representation:\n",
    "Each token is assigned a unique vector from this matrix, trained to capture the token's meaning, context, and relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([2, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example: Vocabulary size = 5000, Embedding dimension = 300\n",
    "embedding_layer = nn.Embedding(num_embeddings=5000, embedding_dim=300)\n",
    "\n",
    "# Sample input tokens (batch size = 2, sequence length = 4)\n",
    "input_tokens = torch.tensor([[1, 45, 203, 4], \n",
    "                             [12, 344, 23, 654]])\n",
    "\n",
    "# Forward pass to get embeddings\n",
    "embeddings = embedding_layer(input_tokens)\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)  # Output: torch.Size([2, 4, 300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "The nn.Embedding() layer maps each token (word index) to a dense vector representation.\n",
    "Here, both the input dimension and the embedding dimension are set to vocab_size.\n",
    "This unusual choice means that each token is effectively mapped to a one-hot encoded vector during training.\n",
    "‚úÖ Why vocab_size as the embedding size?\n",
    "\n",
    "Each token's embedding directly represents logits (raw scores) for predicting the next token.\n",
    "Instead of adding an extra linear layer, this design simplifies prediction for the next token.\n",
    "\n",
    "\n",
    "2) def forward(self, index, targets):\n",
    "    logits = self.token_embedding_table(index)  # Shape: [B, T, C]\n",
    "\n",
    "index ‚Üí Tensor of token indices (shape [B, T], where B = batch size, T = sequence length).\n",
    "The nn.Embedding() layer outputs a 3D tensor of shape [B, T, C] where:\n",
    "B = Batch size\n",
    "T = Sequence length\n",
    "C = Number of classes (equals vocab_size)\n",
    "‚û°Ô∏è Each position's embedding directly represents the logits for predicting the next token.\n",
    "\n",
    "\n",
    "\n",
    "3)B, T, C = logits.shape\n",
    "logits = logits.view(B * T, C)\n",
    "targets = targets.view(B * T)\n",
    "\n",
    "\n",
    "The logits are reshaped from [B, T, C] to [B * T, C] to flatten the batch and time dimensions for easier loss calculation.\n",
    "The targets are similarly reshaped to [B * T].\n",
    "‚úÖ Why Reshape?\n",
    "\n",
    "F.cross_entropy() expects inputs in the shape [N, C] where N is the total number of predictions and C is the number of classes.\n",
    "\n",
    "\n",
    "4)loss = F.cross_entropy(logits, targets)\n",
    "F.cross_entropy() is used for classification tasks. It combines both:\n",
    "LogSoftmax (to convert logits into probabilities).\n",
    "Negative Log Likelihood Loss (to compute the actual loss).\n",
    "This step trains the model to predict the next token given the current one.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "block_size=8\n",
    "batch_size=4\n",
    "max_iters=10000\n",
    "learning_rate=3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Embedding layer maps each token to a vector of size 'vocab_size'\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        # Look up embeddings for the input tokens\n",
    "        logits = self.token_embedding_table(index)  # Shape: [B, T, C]\n",
    "        \n",
    "        if targets is None:\n",
    "           loss = None\n",
    "        else:\n",
    "            # Reshape logits for cross-entropy loss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # Flatten for batch processing\n",
    "        \n",
    "            # Reshape targets to match logits shape\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            #we can also use reshape but they both have their own advantages\n",
    "            #view() reshapes the tensor to match PyTorch's F.cross_entropy() input requirements.\n",
    "        \n",
    "            # Compute the loss using CrossEntropyLoss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits ,loss\n",
    "    \n",
    "    def generate(self,index,max_new_tokens):\n",
    "        \n",
    "        # index ‚Üí Initial sequence of token indices (starting prompt).\n",
    "        # max_new_tokens ‚Üí Number of new tokens to generate.\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "        # The loop runs max_new_tokens times, generating one new token at each step.\n",
    "            \n",
    "            # Get the logits for the last token in the sequence\n",
    "            logits ,loss = self.forward(index)  # Shape: [B, T, C]\n",
    "            logits = logits[:, -1, :]  # Select the last token only (shape [B, C])\n",
    "            \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # Shape: [B, C]\n",
    "            \n",
    "            # Sample the next token from the probability distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # Shape: [B, 1]\n",
    "            \n",
    "            \n",
    "            # Append the predicted token to the sequence\n",
    "            index = torch.cat([index, next_token], dim=1)  # Shape: [B, T+1]\n",
    "\n",
    "        return index\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gK-E/\"WOICqsn83c5O#/\\_'D3xPt\\^Q!])s,{XDxY{|,hVE.^<1v1+KMMY:^1IHMs6+e04o [;G^#cQ=AY%FAyoG}<sz]a!ciG9nqv(/w/ m#UH+q31,{C=RmzQ&g$%eylcv9.C1q!E@{`)o.t*^.0:\\ftkg=4gLw@6.^%4E?0\"RsSu=oT0c7aHW@eY:<[Z{}<%'v}'om.G`8a\"Fq4nl7td!7ceOb11-CP{)CmbM*uz4,k[qn B!ao\"&*TlUJu4grXl0'(?|7 Z1Kb@y|/o;!7p|L-t4s8fSm<'htXv_**pP2$c=0\\EUvN(Ese,dheg<M@$f] /(DZy5HJ5Gl8>58o0Rb?=LfQ{;9Q=Yh|,qby64+Jep&g)>H3(+bDlfHf;858lfJJ,65CCZT'_LNwGpzRf({bWe-_#v'2RWV6t=vFkTx41,5OqKogBj^5d+;Mt@`5A<NLP1Yw'_'4GfB;O{c85|N]T/v9w?.666u4q$-qu\\FA\"Wlm1<\n"
     ]
    }
   ],
   "source": [
    "model= BigramLanguageModel(vocab_size=500)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1) , dtype=torch.long , device=device)\n",
    "generated_chars = decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index in int_to_string: 91\n",
      "Generated token sequence: [0, 462, 156, 110, 33, 325, 329, 88, 205, 490, 224, 150, 52, 74, 131, 355, 422, 331, 294, 211, 255, 400, 364, 113, 246, 357, 89, 221, 57, 7, 351, 321, 226, 360, 468, 293, 434, 308, 148, 362, 252, 240, 207, 487, 138, 85, 443, 115, 36, 49, 183, 12, 88, 464, 39, 307, 392, 150, 54, 274, 393, 335, 235, 294, 383, 8, 399, 230, 477, 315, 126, 27, 260, 113, 242, 191, 124, 317, 139, 386, 191, 294, 63, 434, 303, 372, 247, 362, 41, 41, 238, 81, 83, 38, 466, 165, 210, 209, 172, 399, 308, 408, 127, 206, 274, 376, 257, 474, 479, 291, 92, 93, 373, 174, 77, 477, 348, 408, 98, 30, 205, 207, 44, 110, 129, 21, 272, 78, 215, 192, 299, 41, 198, 436, 125, 498, 444, 3, 249, 491, 195, 445, 81, 263, 441, 435, 194, 85, 87, 452, 472, 115, 214, 75, 32, 123, 309, 74, 95, 65, 14, 100, 18, 162, 477, 12, 415, 493, 75, 388, 228, 193, 173, 36, 338, 480, 435, 139, 370, 189, 413, 371, 283, 490, 178, 476, 54, 208, 81, 217, 209, 432, 185, 70, 14, 175, 89, 69, 235, 281, 262, 421, 85, 486, 206, 144, 76, 322, 52, 402, 415, 195, 106, 340, 205, 22, 429, 488, 362, 133, 108, 364, 318, 197, 422, 180, 110, 20, 152, 1, 160, 344, 144, 25, 484, 5, 90, 400, 461, 46, 377, 376, 361, 184, 65, 210, 56, 448, 240, 242, 301, 98, 70, 119, 314, 298, 140, 0, 93, 317, 53, 151, 278, 467, 250, 18, 436, 52, 455, 215, 133, 429, 277, 255, 442, 480, 216, 124, 244, 187, 465, 388, 182, 85, 205, 185, 136, 283, 71, 126, 486, 478, 187, 451, 495, 77, 141, 302, 283, 466, 370, 177, 307, 320, 417, 210, 449, 246, 23, 402, 280, 151, 277, 301, 79, 68, 142, 367, 350, 112, 352, 112, 85, 56, 282, 449, 330, 56, 213, 169, 244, 22, 105, 464, 302, 1, 37, 263, 173, 175, 66, 47, 434, 209, 147, 459, 333, 96, 226, 11, 14, 3, 412, 104, 32, 279, 8, 88, 3, 148, 67, 339, 285, 189, 5, 63, 441, 47, 492, 182, 111, 73, 142, 401, 199, 366, 454, 361, 160, 23, 379, 248, 452, 424, 151, 219, 416, 457, 27, 117, 227, 117, 347, 414, 137, 19, 279, 235, 284, 410, 192, 262, 17, 138, 401, 492, 186, 444, 215, 10, 20, 6, 70, 129, 154, 407, 234, 202, 375, 477, 439, 315, 447, 287, 31, 255, 26, 323, 447, 82, 118, 410, 358, 389, 1, 18, 377, 93, 0, 307, 206, 151, 18, 433, 71, 455, 468, 222, 73, 112, 439, 57, 62, 199, 431, 132, 258, 192, 148, 311, 176, 490, 426, 240, 76, 51, 131, 311, 13, 124, 398, 437, 132, 32, 228, 427, 465, 17, 397, 429, 313, 295, 306, 461, 376, 191, 498, 75, 419, 447, 366, 88, 243, 219, 466, 179, 51, 367, 379, 95, 308]\n"
     ]
    }
   ],
   "source": [
    "print(\"Max index in int_to_string:\", max(int_to_string.keys()))\n",
    "print(\"Generated token sequence:\", m.generate(context, max_new_tokens=500)[0].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) logits = self.token_embedding_table(index)  # Shape: [B, T, C]\n",
    "logits = logits[:, -1, :]  # Shape: [B, C]\n",
    "\n",
    "The model generates logits for all tokens in the sequence ([B, T, C]).\n",
    "Since we're only interested in the next token prediction, .select() or slicing with [:, -1, :] extracts the logits for the last token in the sequence.\n",
    "The shape becomes [B, C], where:\n",
    "B = Batch size\n",
    "C = Vocabulary size (each class corresponds to a token in the vocabulary).\n",
    "\n",
    "2) probs = torch.softmax(logits, dim=-1)  # Shape: [B, C]\n",
    "\n",
    "Softmax converts the logits into a probability distribution over the vocabulary.\n",
    "Each value now represents the likelihood of a particular token being the next one.\n",
    "\n",
    "3) next_token = torch.multinomial(probs, num_samples=1)  # Shape: [B, 1]\n",
    "torch.multinomial() samples one token based on the generated probabilities.\n",
    "This introduces randomness, making the text more natural and varied.\n",
    "If you want deterministic predictions, you could use .argmax() instead:\n",
    "\n",
    "next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "4)index = torch.cat([index, next_token], dim=1)  # Shape: [B, T+1]\n",
    "The newly generated token is added to the original sequence using torch.cat().\n",
    "This updated index is fed back into the loop for the next prediction.\n",
    "\n",
    "5)return index\n",
    "\n",
    "After generating max_new_tokens, the final sequence (including the newly added tokens) is returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# |\n",
    "# |\n",
    "# |\n",
    "# |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> model = BigramLanguageModel(vocab_size=500)\n",
    "m = model.to(device)\n",
    "\n",
    "BigramLanguageModel(vocab_size=500) ‚Üí Initializes the model with a vocabulary size of 500.\n",
    "\n",
    "Each token's embedding will have 500 dimensions.\n",
    "The embedding table is also trained to predict the next token in the sequence.\n",
    "\n",
    "-->Step 2: Creating the context Tensor\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "‚úÖ This creates a tensor filled with zeros:\n",
    "\n",
    "context = tensor([[0]])\n",
    "(1, 1) ‚Üí A batch size of 1 and a sequence length of 1 (starting prompt is a single token).\n",
    "dtype=torch.long ‚Üí Since token indices are integers, long is used.\n",
    "device=device ‚Üí Ensures the tensor is on the same device as the model.\n",
    "üß† Why Use 0 as the Starting Token?\n",
    "\n",
    "The starting token is often zero when no prompt is provided.\n",
    "It's a placeholder token, and the model will rely on learned embeddings to generate meaningful text from there.\n",
    "\n",
    "\n",
    "-->Step 3: Generating Tokens\n",
    "\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "Inside generate()\n",
    "The context tensor is passed into m.generate() with max_new_tokens=500.\n",
    "The model will predict and generate 500 additional tokens.\n",
    "[0] ‚Üí Extracts the sequence from the first batch (since batch size is 1).\n",
    ".tolist() ‚Üí Converts the tensor to a Python list for easier decoding.\n",
    "\n",
    "\n",
    "-->Example Output from .generate()\n",
    "tensor([[0, 123, 98, 45, 200, 302, ... , 76]])\n",
    "\n",
    "The sequence now contains the starting token 0 followed by 500 generated tokens.\n",
    "\n",
    "\n",
    "-->Step 4: Decoding the Generated Tokens\n",
    "\n",
    "generated_chars = decode(...)\n",
    "‚úÖ The decode() function likely converts the list of token indices back into readable text.\n",
    "\n",
    "Example of decode() function logic:\n",
    "\n",
    "\n",
    "def decode(token_list):\n",
    "    return ''.join([vocab[idx] for idx in token_list])\n",
    "The vocab variable would be a mapping like:\n",
    "\n",
    "vocab = {0: \"<START>\", 1: \"H\", 2: \"e\", 3: \"l\", 4: \"o\", ...}\n",
    "For example:\n",
    "\n",
    "[0, 1, 2, 3, 3, 4] ‚Üí \"<START>Hello\"\n",
    "\n",
    "--> Step 5: Printing the Result\n",
    "\n",
    "print(generated_chars)\n",
    "‚úÖ Displays the generated text output, which could look something like:\n",
    "\n",
    "\"Hello world! This is a sample text generated by the Bigram model...\"\n",
    "\n",
    "\n",
    "--> Complete Flow Summary\n",
    "Model Initialization: The model is created with vocab_size = 500.\n",
    "Context Creation: A starting token (0) is provided.\n",
    "Token Generation: The generate() method repeatedly predicts and appends new tokens for 500 steps.\n",
    "Decoding: The sequence of token indices is converted into readable text.\n",
    "Output Display: The generated text is printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts to Remember\n",
    "‚úÖ logits[:, -1, :] extracts the last token's logits for predicting the next token.\n",
    "‚úÖ torch.softmax() converts logits into probabilities.\n",
    "‚úÖ torch.multinomial() samples tokens randomly, adding diversity to the generated text.\n",
    "‚úÖ torch.cat() appends the predicted token to the existing sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7047104835510254\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    #sample a batch of data\n",
    "    xb,yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)   # Move data to GPU\n",
    "    \n",
    "    #evaluate the loss\n",
    "    \n",
    "    logits , loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
